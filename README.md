

<!-- # DEEM -->

# DEEM: Diffusion models serve as the eyes of large language models for image perception

<div align="center">
<br>
<a href="https://scholar.google.com/citations?user=phg8yxoAAAAJ&hl=zh-CN&oi=ao">Run Luo</a><sup><span>1,2*</span></sup>, 
<a href="https://scholar.google.com/citations?user=juR8ZS4AAAAJ&hl=zh-CN">Yunshui Li</a><sup><span>1,2*</span></sup>,
<a href="https://github.com/October2001">Longze Chen</a><sup><span>1,2*</span></sup>,
<a>Wanwei He</a><sup><span>1,2</span></sup>,
<a>Ting-En Lin</a><sup><span>5</span></sup>,
<a>Ziqiang Liu<sup><span>1,2</span></sup>,
<a>Lei Zhang<sup><span>1,2</span></sup>
<br>
<a>Zikai Song<sup><span>6</span></sup>,
<a>Xiaobo Xia<sup><span>4</span></sup>,
<a>Tongliang Liu<sup><span>4</span></sup>,
<a>Min Yang<sup><span>1,2ğŸŒŸ</span></sup>,
<a>Binyuan Hui<sup><span>3ğŸŒŸ</span></sup>
<br>
    
\* Equal contribution ğŸŒŸ Corresponding author

<sup>1</sup> Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences<br>
<sup>2</sup> University of Chinese Academy of Sciences<br>
<sup>3</sup> Alibaba Group
<sup>4</sup> The University of Sydney 
<sup>5</sup> Tsinghua University
<sup>6</sup> HUST<br>
    
![Multi-Modal](https://img.shields.io/badge/Task-Multi--Modal-red) <a href='https://arxiv.org/pdf/2405.15232'><img src='https://img.shields.io/badge/Paper-Arxiv-red'></a> <a href='https://huggingface.co/collections/yifanzhang114/slime-665bcb2d0d71762b86fdbd2d'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Models-blue'></a> <a href='https://huggingface.co/datasets/yifanzhang114/SMR'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Data-green'></a>

</div>

<p align="center">
    <img src="document/overall.png" width="100%" height="100%">
</p>

<font size=5><div align='center' >  [[ğŸ“– arXiv Paper](https://arxiv.org/pdf/2405.15232)] [[ğŸ“Š Dataset](https://github.com/RainBowLuoCS/DEEM)] [[ğŸ† Models](https://github.com/RainBowLuoCS/DEEM)]  </div></font>
DEEM is an exploration of using diffusion models as the eyes of multi-modal large language models, with the goal of eliminating potential biases in different visual encoders from a vision-centric perspective. We hope that DEEM can bring some thinking to the multimodal community, whether the unbiased diffusion model can replace the traditional visual encoder and become the second unified multimodal structure besides self-regression.

## ğŸ”¥ Update

- [07/21]ğŸ”¥DEEM is coming! We release the [code](https://github.com/RainBowLuoCS/DEEM), [models](https://github.com/RainBowLuoCS/DEEM), and [data](https://github.com/RainBowLuoCS/DEEM) for DEEM!
- [07/05]ğŸ”¥DEEM is coming! We release the [paper](https://arxiv.org/abs/2405.15232) for DEEM!

## ğŸ‘€ Contents

- [Setup](#Setup)
- [Model](#model)
- [Preparation](#preparation)
- [Train](#train)
- [Evaluation](#evaluation)
- [Examples](#examples)
- [Citation](#citation)


## ğŸ“· Setup

Please follow the instructions below to install the required packages.


1. Clone this repository

```bash
https://github.com/RainBowLuoCS/DEEM.git
```

2. Install Package

```bash
conda create -n deem python=3.10 -y
conda activate deem
cd DEEM
pip install -r requirements.txt
# install `MultiScaleDeformableAttention` module
cd uni_interleaved/models/utils/ops
python setup.py install
```

3. Download all pretrained model components from huggingface into the `assets/` directory by running the following commands:

```bash
python scripts/download_models.py
```

## ğŸ” Model

Here are the pretrained weights on Stage 1 data only:

| Model                        | Diffusion Model | Base LLM       | Vision Encoder | Pretrain Data | Download |
| ---------------------------- | --------------- | -------------- | -------------- | ------------- | -------- |
| MM-interleaved-7B (Baseline) | SD 2.1 ğŸ”¥        | Vicuna-7B-v1.5 | ConvNext-B     | MMC4+LAION    | [ckpt]() |
| DEEM-7B                      | SD 2.1          | Vicuna-7B-v1.5 | ConvNext-BğŸ”¥    | MMC4+LAION    | [ckpt]() |
| (DEEM+MM-interleaved)-7B     | SD 2.1 ğŸ”¥        | Vicuna-7B-v1.5 | ConvNext-BğŸ”¥    | MMC4+LAION    | [ckpt]() |

We provide all our fully finetuned models on Stage 2 and 3 data for DEEM:

| Model         | Base LLM       | Vision Encoder | Finetuning Data       | Download |
| ------------- | -------------- | -------------- | --------------------- | -------- |
| DEEM-VQA 7B   | Vicuna-7B-v1.5 | ConvNext-B     | LLaVA-665k+VQA+COCO   | [ckpt]() |
| DEEM-MASK 7B  | Vicuna-7B-v1.5 | ConvNext-B     | ReferCOCO+VG+PartData | [ckpt]() |

## ğŸ’¡ Preparation

### Dataset

Please follow [MM-Interleaved](https://github.com/OpenGVLab/MM-Interleaved) , [LLaVA](https://github.com/haotian-liu/LLaVA) and [Osprey](https://github.com/CircleRadon/Osprey) to prepare the corresponding images and data.

### DEEM data structure

```
datasets
â”œâ”€â”€ laion
â”‚   â”œâ”€â”€ laion_annts
â”‚   â”‚   â””â”€â”€ laion_shard_{0...1774}_v1.json
â”‚   â””â”€â”€ laion_images
â”‚   â”‚   â””â”€â”€ {00000..01174}.tar
â”œâ”€â”€ mmc4
â”‚   â”œâ”€â”€ mmc4_annts
â”‚   â”‚   â””â”€â”€ docs_no_face_shard_{0..23098}_v3.jsonl
â”‚   â””â”€â”€ mmc4_images
â”‚   â”‚   â””â”€â”€ b9040a0dbb22.jpg
â”œâ”€â”€ aokvqa
â”‚   â””â”€â”€ aokvqa_v1p0_train.json
â”œâ”€â”€ image2parag
â”‚   â”œâ”€â”€ paragraphs_coco.json
â”‚   â”œâ”€â”€ paragraphs_v1.json
â”‚   â”œâ”€â”€ test_split.json
â”‚   â”œâ”€â”€ train_split.json
â”‚   â””â”€â”€ val_split.json
â”œâ”€â”€ coco
â”‚   â”œâ”€â”€ train2014 
â”‚   â”œâ”€â”€ train2017
â”‚   â”œâ”€â”€ val2014
â”‚   â”œâ”€â”€ val2017
â”‚   â””â”€â”€ annotations
â”‚   â”‚   â”œâ”€â”€ coco_karpathy_train.json    
â”‚   â”‚   â”œâ”€â”€ coco_karpathy_val.json  
â”‚   â”‚   â”œâ”€â”€ captions_train2017.json          
â”‚   â”‚   â”œâ”€â”€ coco_karpathy_val_gt.json         
â”‚   â”‚   â”œâ”€â”€ coco_karpathy_test.json   
â”‚   â”‚   â”œâ”€â”€ instances_train2017
â”‚   â”‚   â””â”€â”€ coco_karpathy_test_gt.json
â”œâ”€â”€ lncoco
â”‚   â”œâ”€â”€ coco_train_captions.jsonl
â”‚   â””â”€â”€ coco_val_captions.jsonl
â”œâ”€â”€ flickr30k
â”‚   â”œâ”€â”€ flickr30k-images  
â”‚   â”œâ”€â”€ captiontobbox.json 
â”‚   â”œâ”€â”€ flickr30k_test1k.json
â”‚   â”œâ”€â”€ phrasetobbox.json
â”‚   â””â”€â”€ groundedcaption.json
â”œâ”€â”€ gqa
â”‚   â”œâ”€â”€ images  
â”‚   â”œâ”€â”€ test_balanced_questions.json
â”‚   â”œâ”€â”€ train_balanced_questions.json
â”‚   â””â”€â”€ testdev_balanced_questions.json 
â”œâ”€â”€ robustvqa
â”‚   â”œâ”€â”€ imagenet-r 
â”‚   â”œâ”€â”€ imagenet-a
â”‚   â”œâ”€â”€ imagenetv2
â”‚   â””â”€â”€ robustvqa_test.json
â”œâ”€â”€ llava
â”‚   â””â”€â”€ llava_v1_5_mix665k.json
â”œâ”€â”€ nocaps
â”‚   â”œâ”€â”€ val_imgs
â”‚   â””â”€â”€ nocaps_val_4500_captions.json
â”œâ”€â”€ ocr_vqa
â”‚   â”œâ”€â”€ images 
â”‚   â””â”€â”€ dataset.json
â”œâ”€â”€ okvqa
â”‚   â”œâ”€â”€ OpenEnded_mscoco_train2014_questions.json
â”‚   â”œâ”€â”€ OpenEnded_mscoco_val2014_questions.json 
â”‚   â”œâ”€â”€ mscoco_train2014_annotations.json
â”‚   â””â”€â”€ mscoco_val2014_annotations.json
â”œâ”€â”€ part_data
â”‚   â”œâ”€â”€ test.json
â”‚   â”œâ”€â”€ train.json
â”‚   â”œâ”€â”€ val.json
â”‚   â”œâ”€â”€ partImagenet 
â”‚   â”‚   â”œâ”€â”€ train         
â”‚   â”‚   â””â”€â”€ partImagenet_train_format.json
â”‚   â””â”€â”€ pascal_part
â”‚   â”‚   â”œâ”€â”€ VOCdevkit   
â”‚   â”‚   â””â”€â”€ pascalpart_train.json
â”œâ”€â”€ pope
â”‚   â”œâ”€â”€ coco_pope_adversarial.json
â”‚   â”œâ”€â”€ coco_pope_popular.json
â”‚   â””â”€â”€ coco_pope_random.jso
â”œâ”€â”€ refcoco
â”‚   â”œâ”€â”€ refcoco+
â”‚   â”œâ”€â”€ refcoco
â”‚   â”œâ”€â”€ refcocog
â”‚   â”œâ”€â”€ finetune_refcoco+_train_with_mask.json 
â”‚   â”œâ”€â”€ finetune_refcoco_train_with_mask.json
â”‚   â””â”€â”€ finetune_refcocog_val_with_mask.json
â”œâ”€â”€ textcaps
â”‚   â”œâ”€â”€ TextCaps_0.1_train.json
â”‚   â””â”€â”€ TextCaps_0.1_val.json
â”œâ”€â”€ textvqa
â”‚   â”œâ”€â”€ train_images
â”‚   â”œâ”€â”€ TextVQA_0.5.1_train.json
â”‚   â”œâ”€â”€ textvqa_val_annotations.json
â”‚   â”œâ”€â”€ TextVQA_0.5.1_val.json
â”‚   â””â”€â”€ textvqa_val_questions.json 
â”œâ”€â”€ vcr
â”‚   â”œâ”€â”€ vcr1images
â”‚   â”œâ”€â”€ test.jsonl
â”‚   â”œâ”€â”€ train.jsonl
â”‚   â”œâ”€â”€ val.jsonl
â”‚   â””â”€â”€ textvqa_val_questions.json 
â”œâ”€â”€ vg
â”‚   â”œâ”€â”€ VG_100K
â”‚   â”œâ”€â”€ VG_100k_2
â”‚   â”œâ”€â”€ region_descriptions.json
â”‚   â”œâ”€â”€ image_data.json
â”‚   â”œâ”€â”€ vg_train_with_mask.json
â”‚   â””â”€â”€ question_answers.json 
â”œâ”€â”€ visdial
â”‚   â”œâ”€â”€ VisualDialog_val2018
â”‚   â”œâ”€â”€ visdial_1.0_val_dense_annotations.json
â”‚   â””â”€â”€ visdial_1.0_val.json  
â”œâ”€â”€ vizwiz
â”‚   â”œâ”€â”€ val
â”‚   â”œâ”€â”€ test.json
â”‚   â”œâ”€â”€ train.json
â”‚   â””â”€â”€ val.json 
â””â”€â”€ vqav2
â”‚   â”œâ”€â”€ v2_OpenEnded_mscoco_train2014_questions.json
â”‚   â”œâ”€â”€ v2_OpenEnded_mscoco_val2014_questions.json
â”‚   â”œâ”€â”€ v2_mscoco_train2014_annotations.json
â”‚   â””â”€â”€ v2_mscoco_val2014_annotations.json 

```

We provide two very convenient scripts to download a large amount of pre-training data, laion and mmc4. You can download the pre-training data by running the following scripts. Due to network reasons, only 40% of the data can be obtained in the end.

```bash
python ./scripts/download_laion.py --mode=annt
python ./scripts/download_mmc4.py --mode=annt
python ./scripts/download_laion.py --mode=images
python ./scripts/download_mmc4.py --mode=images
```

You can find all dataset downloading and convertaion scripts or information in `scripts/`

Note that after downloading the mmc4 dataset, you need to use the following conversion script to convert it into the pre-trained format. Use the corresponding version according to the download method you choose.

```bash
python ./scripts/convert_mmc4_for_pretrain.py
```

You can use the robustvqa file we provide in datasets or regenerate it yourself using the script

```bash
python ./scripts/convert_imagenet_for_robustvqa_test.py
```


## ğŸ“ˆ Train

<div align='center' >
<details>
<summary> Click to see the detail model structure</summary>
<p align="center">
<img width="100%" src="document/framework.png"/>
</details>
</div>

DEEM training consists of three stages: (1) image-text alignment  pre-training; (2) image-text supervised fine-tuning ; and (3) mask-text supervised fine-tuning.

DEEM is trained on 32 A100 GPUs with 80GB memory. To train on fewer GPUs, you can reduce the `per_device_train_batch_size` and increase the `gradient_accumulation_steps` accordingly. Always keep the global batch size the same: `per_device_train_batch_size` x `gradient_accumulation_steps` x `num_gpus`.

Please make sure you download and organize the data following [Preparation](#preparation) before training and evaluation.

```bash
bash scripts/train.sh
```


## ğŸ“ˆ Evaluation

We perform evaluation on several image-based benchmarks. Please see our paper for the more details.

<div align=center>
<img width="100%" src="document/evaluation.jpg"/>
</div>

If you want to evaluate the model on image-based benchmarks, please use the evaluation scripts  for automatic evaluation.

```bash
bash scripts/evaluate.sh
```

For gqa we need to use the following script separately for evaluation.

```bash
unzip -d ./uni_interleaved/utils/gqa_metrics_src/ ./uni_interleaved/utils/gqa_metrics_src/train_choices.zip
python ./uni_interleaved/utils/gqa_eval.py
```

## ğŸ‘€ Examples

We provide some examples in this section. More examples can be found in our paper

<div align=center>
<img width="90%" src="document/case1.png"/>
</div>

<div align='center' >
<details>
<summary> Click to expand more examples</summary>
<p align="center">
    <img src="document/case2.png" width="60%" height="60%">
    <img src="document/case3.png" width="60%" height="60%">
    <img src="document/case4.png" width="60%" height="60%">
    <img src="document/case5.png" width="60%" height="60%">
</details>
</div>

## Schedule

- [x] Release training & evaluation code

- [x] Release stage 1 image-text alignment  pre-training  model weights

- [ ] Release stage 2 image-text sft  model weights

- [ ] Release stage 3 mask-text sft  model weights

## Citation

If you find this repo useful for your research, please consider citing the paper

```
@article{luo2024deem,
  title={Deem: Diffusion models serve as the eyes of large language models for image perception},
  author={Luo, Run and Li, Yunshui and Chen, Longze and He, Wanwei and Lin, Ting-En and Liu, Ziqiang and Zhang, Lei and Song, Zikai and Xia, Xiaobo and Liu, Tongliang and others},
  journal={arXiv preprint arXiv:2405.15232},
  year={2024}
}
```

## Acknowledgement

We would like to thank the following repos for their great work:

- This work is built upon the[MM-Interleaved](https://github.com/OpenGVLab/MM-Interleaved)
- This work utilizes LLMs from , [Vicuna](https://github.com/lm-sys/FastChat)
- This work utilizes the great work from [OpenFlamingo](https://github.com/mlfoundations/open_flamingo), [transformers](https://github.com/huggingface/transformers), [diffusers](https://github.com/huggingface/diffusers), [LLaMA](https://github.com/facebookresearch/llama), [CLIP](https://github.com/openai/CLIP), [BLIP](https://github.com/salesforce/BLIP), [ViT-Adapter](https://github.com/czczup/ViT-Adapter) and [Osprey](https://github.com/CircleRadon/Osprey).

## License

This project is released under the [Apache 2.0 license](https://github.com/RainBowLuoCS/DEEM/blob/main/LICENSE). Parts of this project contain code and models from other sources, which are subject to their respective licenses.

